{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Acceralation of word2vec\n",
    "\n",
    "The problem of word2vec which implemented previous chapter is that it would be slow if the courpus is huge. So, we need to improve word2vec.\n",
    "\n",
    "## 4.1 word2vec improvement : Embedding class\n",
    "\n",
    "forward method of Embedding class just extract the word vector which associated  with current id from the weight matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "[6 7 8]\n",
      "[[3 4 5]\n",
      " [0 1 2]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "\n",
    "print(W)\n",
    "print(W[2])\n",
    "\n",
    "idx = np.array([1, 0, 2])\n",
    "print(W[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        # np.add.at(dW, self.idx, dout)\n",
    "        # return None\n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            dW[word_id] += dout[i]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 word2vec improvement : Negative sampling\n",
    "\n",
    "### 4.2.1 calculation problem after middle layer\n",
    "\n",
    "When the corpus size is huge, calculation takes time in following process.\n",
    "\n",
    "- the product of the neurons of the hidden layer and the weight matrix\n",
    "- calculation of Softmax layer\n",
    "\n",
    "For example, softmax is shown as following assuming that the corpus size is 100,000.\n",
    "\n",
    "$$\n",
    "y = \\frac{\\exp(x_i)}{\\sum_j^{1000000} \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "### 4.2.2 Negative sampling\n",
    "\n",
    "To reduce the calculation problem, we use negative sampling in which we randomly select negative samples from the corpus.\n",
    "\n",
    "### 4.2.3 Sigmoid function and Cross entropy error\n",
    "\n",
    "Sigmoid function and Cross entropy error are shown as following.\n",
    "\n",
    "Sigmoid:\n",
    "$$\n",
    "y = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Cross entropy error:\n",
    "$$\n",
    "L = -(t \\log y + (1 - t) \\log (1 - y))\n",
    "$$\n",
    "\n",
    "![../images/4.2.3.png](./fig/4_2_3.drawio.svg)\n",
    "\n",
    "### 4.2.4 From multiclass classification to binary classification\n",
    "\n",
    "![img](./fig/4_2_4.drawio.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding dot class\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5]\n",
      " [122]\n",
      " [ 86]]\n",
      "[[ 0  1  2]\n",
      " [ 9 10 11]\n",
      " [ 3  4  5]]\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "h = np.arange(9).reshape(3, 3)\n",
    "idx = np.array([0, 3, 1])\n",
    "\n",
    "embed = Embedding(W)\n",
    "target_W = embed.forward(idx)\n",
    "out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "print(out.reshape(out.shape[0], 1))\n",
    "print(target_W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 Negative sampling\n",
    "\n",
    "### 4.2.6 Sampling method of Negative sampling\n",
    "\n",
    "\n",
    "$$\n",
    "P'(W_i) = \\frac{P(W_i)^{0.75}}{\\sum_j^{n} P(W_j)^{0.75}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
