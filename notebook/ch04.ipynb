{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Acceralation of word2vec\n",
    "\n",
    "The problem of word2vec which implemented previous chapter is that it would be slow if the courpus is huge. So, we need to improve word2vec.\n",
    "\n",
    "## 4.1 word2vec improvement : Embedded class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "[6 7 8]\n",
      "[[3 4 5]\n",
      " [0 1 2]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "\n",
    "print(W)\n",
    "print(W[2])\n",
    "\n",
    "idx = np.array([1, 0, 2])\n",
    "print(W[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        # np.add.at(dW, self.idx, dout)\n",
    "        # return None\n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            dW[word_id] += dout[i]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 word2vec improvement : Negative sampling\n",
    "\n",
    "### 4.2.1 calculation problem after middle layer\n",
    "\n",
    "When the corpus size is huge, calculation takes time in following process.\n",
    "\n",
    "- the product of the neurons of the hidden layer and the weight matrix\n",
    "- calculation of Softmax layer\n",
    "\n",
    "For example, softmax is shown as following assuming that the corpus size is 100,000.\n",
    "\n",
    "$$\n",
    "y = \\frac{\\exp(x_i)}{\\sum_j^{1000000} \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "### 4.2.2 Negative sampling\n",
    "\n",
    "To reduce the calculation problem, we use negative sampling in which we randomly select negative samples from the corpus.\n",
    "\n",
    "### 4.2.3 Sigmoid function and Cross entropy error\n",
    "\n",
    "ToDo...\n",
    "\n",
    "### 4.2.6 Sampling method of negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
