{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 word2vec\n",
    "\n",
    "## 3.1 Estimation based method and Nerural Network\n",
    "\n",
    "### 3.1.1 Problems of count based method\n",
    "\n",
    "The count based method expresses the word by the word that appears around it.  \n",
    "To create huge size of corpus, it will include over 100 Million terms.  \n",
    "It will take so much time to compute.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20241309 -1.31116949  0.68082365]]\n"
     ]
    }
   ],
   "source": [
    "# 3.1.3 Word Processing of Neural network\n",
    "import numpy as np\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "W = np.random.randn(7, 3)\n",
    "h = np.dot(c, W)\n",
    "\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.50664811 0.23032133 1.52703031]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "W = np.random.randn(7, 3)\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implemention of simple word2vec\n",
    "\n",
    "In this Chapter, continuous bag-of-words: CBOW model is used.\n",
    "CBOW is a Neural Network designed to infer a target from context.\n",
    "\n",
    "CBOW model can be shown as:\n",
    "![img](./fig/3_2_1.drawio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-1.0399944   0.98493697  2.30230196  0.8781082   1.48025635\n",
      "     2.80441429 -2.68816612]]]]\n"
     ]
    }
   ],
   "source": [
    "# 3.2.1 Impremention of simple word2vec\n",
    "import numpy as np\n",
    "\n",
    "# sample context data\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "# show result\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Learning of CBOW model\n",
    "\n",
    "Learning model can be shown as:\n",
    "\n",
    "![CBOW](./fig/3_2_2.drawio.svg)\n",
    "\n",
    "From the figure shows in ### 3.2.1, softmax function and cross entropy function are added.\n",
    "Softmax function is used to get probability.\n",
    "Cross entropy function is used to get loss to calculate gradient.\n",
    "\n",
    "### 3.2.3 Weights and distributions expression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Preparation of Learning data\n",
    "\n",
    "### 3.3.1 context data and target data\n",
    "\n",
    "Input of word2vec is context data and target data.\n",
    "Context means the data that appears around the target data.\n",
    "If window size is 1, context data contains two words around the target data one before and one after.\n",
    "\n",
    "In following corpus, contexts and targets are shown as follows.\n",
    "\n",
    "Corpus = \"you say goodbye and I say hello.\"\n",
    "\n",
    "| Contexts | Targets |\n",
    "|---------|---------|\n",
    "| you, goodbye| say|\n",
    "| say, and | goodbye |\n",
    "| goodbye, i | and |\n",
    "| and, say | i |\n",
    "| I, hello | say |\n",
    "| say, . | hello |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "# 3.3.1 context data\n",
    "\n",
    "import sys\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "\n",
    "print(id_to_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Transform to one-hot vector representation\n",
    "\n",
    "```python\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''one-hot表現への変換\n",
    "\n",
    "    :param corpus: 単語IDのリスト(1次元もしくは2次元のNumPy配列)\n",
    "    :param vocab_size: 語彙数\n",
    "    :return: one-hot表現(2次元のNumPy配列)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
