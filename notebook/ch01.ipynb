{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4159484e",
   "metadata": {},
   "source": [
    "# Chapter1 Review of Neural Network\n",
    "\n",
    "## 1.1 Math and Python\n",
    "\n",
    "## 1.2 Prediction of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d90b71ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.10134872 -1.41417644 -1.16212914]\n",
      " [ 2.43040025 -1.29084118 -1.66664124]\n",
      " [ 2.63500701 -1.4937869  -1.69107609]\n",
      " [ 2.37488515 -1.54861461 -1.36167413]\n",
      " [ 1.68977795 -0.48500454 -1.26002134]\n",
      " [ 2.36920073 -1.55192972 -1.35136696]\n",
      " [ 1.03172442  0.87037616 -1.08451248]\n",
      " [ 1.88929268 -0.78081083 -1.33039661]\n",
      " [ 1.93177135 -1.12932423 -1.15879173]\n",
      " [ 0.97835534  0.36948306 -0.83115496]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.random.randn(10, 2)\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "h = np.dot(x, W1) + b1\n",
    "a = sigmoid(h)\n",
    "s = np.dot(a, W2) + b2\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf5a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb354ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3feca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.60663214  1.72760197  1.8463626 ]\n",
      " [-1.50555447  1.5165633   1.51227392]\n",
      " [-1.39396086  0.85685637  1.09181215]\n",
      " [-1.56936315  1.26091425  1.81409576]\n",
      " [-1.62468107  1.71787897  1.91875439]\n",
      " [-1.53733119  1.53035004  1.69287601]\n",
      " [-1.50490494  1.44085546  1.57030694]\n",
      " [-1.40561903  0.77334277  1.17102664]\n",
      " [-1.4519405   1.13861015  1.36879361]\n",
      " [-1.58880335  1.79276497  1.66510344]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3) # input_size, hidden_size, output_size\n",
    "s = model.predict(x)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63797f6f",
   "metadata": {},
   "source": [
    "## 1.3 Laerning of Neural Network\n",
    "\n",
    "### 1.3.1 Loss Function\n",
    "![img](./fig/1_3_1.drawio.svg)\n",
    "\n",
    "softmax function is shown as folloing:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        y_k &= \\frac{\\exp(s_k)}{\\sum_i^n \\exp(s_i)} \\\\\n",
    "        L &= -\\sum_i^n t_k \\log{y_k}\n",
    "    \\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd95a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size:\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbdd5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6844cff7",
   "metadata": {},
   "source": [
    "### 1.3.2 Differentiation and Gradient\n",
    "\n",
    "\n",
    "### 1.3.4 Calculation Graph\n",
    "\n",
    "#### 1.3.4.2 Branch(Copy) Node\n",
    "\n",
    "The result of Backpropation through a branch copy node is the sum of hte gradients from the branched outputs.\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial x} + \\frac{\\partial L}{\\partial x}\n",
    "$$\n",
    "\n",
    "#### 1.3.4.3 Repeat node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c3e29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
